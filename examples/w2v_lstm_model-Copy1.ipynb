{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neal\\Anaconda3\\envs\\keras\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\neal\\Anaconda3\\envs\\keras\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import json\n",
    "import gensim\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas.plotting import scatter_matrix\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.layers import Input, Bidirectional, LSTM, regularizers\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D, MaxPooling2D, Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '../wyns/data/tweet_global_warming.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>existence</th>\n",
       "      <th>existence.confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global warming report urges governments to act...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fighting poverty and global warming in Africa ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.8786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>URUGUAY: Tools Needed for Those Most Vulnerabl...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.8087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet existence  \\\n",
       "0  Global warming report urges governments to act...       Yes   \n",
       "1  Fighting poverty and global warming in Africa ...       Yes   \n",
       "2  Carbon offsets: How a Vatican forest failed to...       Yes   \n",
       "3  Carbon offsets: How a Vatican forest failed to...       Yes   \n",
       "4  URUGUAY: Tools Needed for Those Most Vulnerabl...       Yes   \n",
       "\n",
       "   existence.confidence  \n",
       "0                1.0000  \n",
       "1                1.0000  \n",
       "2                0.8786  \n",
       "3                1.0000  \n",
       "4                0.8087  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(filename, encoding='latin')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "word_vector_model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(txt, vocab=None, replace_char=' ',\n",
    "                max_length=300, pad_out=False,\n",
    "                to_lower=True, reverse = False,\n",
    "                truncate_left=False, encoding=None,\n",
    "                letters_only=False):\n",
    "  \n",
    "    txt = txt.split()\n",
    "    # Remove HTML\n",
    "    # This will keep characters and other symbols\n",
    "    txt = [re.sub(r'http:.*', '', r) for r in txt]\n",
    "    txt = [re.sub(r'https:.*', '', r) for r in txt]\n",
    "    \n",
    "    txt = ( \" \".join(txt))\n",
    "    # Remove non-emoticon punctuation and numbers\n",
    "    txt = re.sub(\"[.,!0-9]\", \" \", txt)\n",
    "    if letters_only: \n",
    "        txt = re.sub(\"[^a-zA-Z]\", \" \", txt)\n",
    "    txt = \" \".join(txt.split())\n",
    "    # store length for multiple comparisons\n",
    "    txt_len = len(txt)\n",
    "\n",
    "    if truncate_left:\n",
    "        txt = txt[-max_length:]\n",
    "    else:\n",
    "        txt = txt[:max_length]\n",
    "    # change case\n",
    "    if to_lower:\n",
    "        txt = txt.lower()\n",
    "    # Reverse order\n",
    "    if reverse:\n",
    "        txt = txt[::-1]\n",
    "    # replace chars\n",
    "    if vocab is not None:\n",
    "        txt = ''.join([c if c in vocab else replace_char for c in txt])\n",
    "    # re-encode text\n",
    "    if encoding is not None:\n",
    "        txt = txt.encode(encoding, errors=\"ignore\")\n",
    "    # pad out if needed\n",
    "    if pad_out and max_length>txt_len:\n",
    "        txt = txt + replace_char * (max_length - txt_len)\n",
    "    if txt.find('@') > -1:\n",
    "        for i in range(len(txt.split('@'))-1):\n",
    "            try:\n",
    "                if str(txt.split('@')[1]).find(' ') > -1:\n",
    "                    to_remove = '@' + str(txt.split('@')[1].split(' ')[0]) + \" \"\n",
    "                else:\n",
    "                    to_remove = '@' + str(txt.split('@')[1])\n",
    "                txt = txt.replace(to_remove,'')\n",
    "            except:\n",
    "                pass\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balance(df):\n",
    "    print(\"Balancing the classes\")\n",
    "    type_counts = df['Sentiment'].value_counts()\n",
    "    min_count = min(type_counts.values)\n",
    "\n",
    "    balanced_df = None\n",
    "    for key in type_counts.keys():\n",
    "\n",
    "        df_sub = df[df['Sentiment']==key].sample(n=min_count, replace=False)\n",
    "        if balanced_df is not None:\n",
    "            balanced_df = balanced_df.append(df_sub)\n",
    "        else:\n",
    "            balanced_df = df_sub\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_to_sentiment(tweet):\n",
    "    norm_text = normalize(tweet[0])\n",
    "    if tweet[1] in ('Yes', 'Y'):\n",
    "        return ['positive', norm_text]\n",
    "    elif tweet[1] in ('No', 'N'):\n",
    "        return ['negative', norm_text]\n",
    "    else:\n",
    "        return ['other', norm_text]\n",
    "    \n",
    "df = pd.read_csv(filename, encoding='latin')\n",
    "data = []\n",
    "for index, row in df.iterrows():\n",
    "    data.append(tweet_to_sentiment(row))\n",
    "        \n",
    "twitter = pd.DataFrame(data, columns=['Sentiment', 'clean_text'], dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090\n"
     ]
    }
   ],
   "source": [
    "# For this demo lets just keep one and five stars the others are marked 'other\n",
    "# twitter = twitter[twitter['Sentiment'].isin(['positive', 'negative'])]\n",
    "twitter.head()\n",
    "print(len(twitter))\n",
    "# twitter['Sentiment'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment                                                                                                               positive\n",
      "clean_text    global warming report urges governments to act|brussels belgium (ap) - the world faces increased hunger and [link]\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 300\n",
    "print(twitter.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to balance training data\n",
    "# twitter = balance(twitter)\n",
    "# len(twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Now go from the pandas into lists of text and labels\n",
    "text = twitter['clean_text'].values\n",
    "labels_0 = pd.get_dummies(twitter['Sentiment'])  # mapping of the labels with dummies (has headers)\n",
    "# print(labels_0[:10], twitter['Sentiment'].iloc[:10])\n",
    "# labels = labels_0.values\n",
    "labels = labels_0.values[:,[0,2]] # removes the headers\n",
    "print(labels)\n",
    "# Perform the Train/test split\n",
    "X_train_, X_test_, Y_train_, Y_test_ = train_test_split(text,labels, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      negative  other  positive\n",
      "0            0      0         1\n",
      "1            0      0         1\n",
      "2            0      0         1\n",
      "3            0      0         1\n",
      "4            0      0         1\n",
      "5            0      0         1\n",
      "6            0      0         1\n",
      "7            0      0         1\n",
      "8            0      0         1\n",
      "9            0      0         1\n",
      "10           0      0         1\n",
      "11           0      0         1\n",
      "12           0      0         1\n",
      "13           0      0         1\n",
      "14           0      1         0\n",
      "15           0      0         1\n",
      "16           0      0         1\n",
      "17           1      0         0\n",
      "18           0      0         1\n",
      "19           0      0         1\n",
      "20           0      0         1\n",
      "21           0      0         1\n",
      "22           0      0         1\n",
      "23           0      0         1\n",
      "24           0      0         1\n",
      "25           0      0         1\n",
      "26           0      0         1\n",
      "27           0      0         1\n",
      "28           0      0         1\n",
      "29           0      0         1\n",
      "...        ...    ...       ...\n",
      "6060         0      1         0\n",
      "6061         0      0         1\n",
      "6062         0      0         1\n",
      "6063         0      0         1\n",
      "6064         0      0         1\n",
      "6065         0      1         0\n",
      "6066         0      1         0\n",
      "6067         0      1         0\n",
      "6068         0      0         1\n",
      "6069         0      1         0\n",
      "6070         0      0         1\n",
      "6071         0      0         1\n",
      "6072         0      1         0\n",
      "6073         0      0         1\n",
      "6074         0      1         0\n",
      "6075         0      0         1\n",
      "6076         0      0         1\n",
      "6077         0      0         1\n",
      "6078         0      0         1\n",
      "6079         0      0         1\n",
      "6080         0      0         1\n",
      "6081         0      0         1\n",
      "6082         0      0         1\n",
      "6083         0      1         0\n",
      "6084         0      0         1\n",
      "6085         0      0         1\n",
      "6086         0      1         0\n",
      "6087         1      0         0\n",
      "6088         1      0         0\n",
      "6089         1      0         0\n",
      "\n",
      "[6090 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(labels_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1218, 2) (844, 2)\n"
     ]
    }
   ],
   "source": [
    "Y_test = []\n",
    "xt = []\n",
    "for i in range(Y_test_.shape[0]):\n",
    "    if Y_test_[i].mean() > 0:\n",
    "        Y_test.append(Y_test_[i])\n",
    "        xt.append(X_test[i])\n",
    "Y_test = np.array(Y_test)\n",
    "xt = np.array(xt)\n",
    "print(Y_test_.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-144-f2cdf694966b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m class_weights = class_weight.compute_class_weight('balanced',\n\u001b[0;32m      4\u001b[0m                                                  \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                                                  Y_train_)\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras\\lib\\site-packages\\sklearn\\utils\\class_weight.py\u001b[0m in \u001b[0;36mcompute_class_weight\u001b[1;34m(class_weight, classes, y)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         raise ValueError(\"classes should include all valid labels that can \"\n\u001b[0;32m     43\u001b[0m                          \"be in y\")\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "# print(Y_train_[:,0].mean(), Y_train_[:,1].mean())\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(Y_train_),\n",
    "                                                 Y_train_)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "### Now for a simple bidirectional LSTM algorithm we set our feature sizes and train a tokenizer\n",
    "# First we Tokenize and get the data into a form that the model can read - this is BoW\n",
    "# In this cell we are also going to define some of our hyperparameters\n",
    "max_fatures = 2000\n",
    "max_len = 2000\n",
    "words_len = 30\n",
    "batch_size = 32\n",
    "embed_dim = 300\n",
    "lstm_out = 140\n",
    "\n",
    "dense_out=len(labels[0]) #length of features\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(X_train_)\n",
    "X_train = tokenizer.texts_to_sequences(X_train_)\n",
    "X_train = pad_sequences(X_train, maxlen=words_len, padding='post')\n",
    "print(X_train[:,-1].mean())\n",
    "X_test = tokenizer.texts_to_sequences(X_test_)\n",
    "X_test = pad_sequences(X_test, maxlen=words_len, padding='post')\n",
    "word_index = tokenizer.word_index\n",
    "# print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(max_fatures, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_len:\n",
    "#         print(i)\n",
    "        continue\n",
    "    # words not found in embedding index will be all-zeros.\n",
    "    if word in word_vector_model.vocab:\n",
    "        embedding_matrix[i] = word_vector_model.word_vec(word)\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = True to fine tune the embeddings\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            embed_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_fatures,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_22 (Embedding)     (None, 2000, 300)         600000    \n",
      "_________________________________________________________________\n",
      "bidirectional_25 (Bidirectio (None, 280)               493920    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 250)               70250     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 1,164,672\n",
      "Trainable params: 564,672\n",
      "Non-trainable params: 600,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model using the pre-trained embedding\n",
    "# import keras\n",
    "sequence_input = Input(shape=(words_len,), dtype='int32')\n",
    "# gbm_input = Input(shape=(Y_train_.shape[1],), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "# embedded_sequences = keras.layers.concatenate([gbm_input,embedding_layer(sequence_input)],axis=1)\n",
    "x = Bidirectional(LSTM(lstm_out, recurrent_dropout=0.5, activation='tanh'))(embedded_sequences)\n",
    "# x = Bidirectional(LSTM(lstm_out, recurrent_dropout=0.5, activation='tanh'))(x)\n",
    "x = Dense(250, activation='elu')(x)\n",
    "preds = Dense(dense_out, activation='softmax')(x)\n",
    "\n",
    "model = Model([sequence_input], preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4872 samples, validate on 844 samples\n",
      "Epoch 1/20\n",
      " - 22s - loss: 0.3415 - acc: 0.5603 - val_loss: 0.4122 - val_acc: 0.7998\n",
      "Epoch 2/20\n",
      " - 16s - loss: 0.2878 - acc: 0.5926 - val_loss: 0.3954 - val_acc: 0.8009\n",
      "Epoch 3/20\n",
      " - 16s - loss: 0.2736 - acc: 0.6096 - val_loss: 0.3754 - val_acc: 0.8140\n",
      "Epoch 4/20\n",
      " - 15s - loss: 0.2486 - acc: 0.6238 - val_loss: 0.3768 - val_acc: 0.8258\n",
      "Epoch 5/20\n",
      " - 15s - loss: 0.2331 - acc: 0.6367 - val_loss: 0.3935 - val_acc: 0.8164\n",
      "Epoch 6/20\n",
      " - 15s - loss: 0.2126 - acc: 0.6486 - val_loss: 0.3995 - val_acc: 0.8235\n",
      "Epoch 7/20\n",
      " - 16s - loss: 0.2047 - acc: 0.6470 - val_loss: 0.3949 - val_acc: 0.8329\n",
      "Epoch 8/20\n",
      " - 16s - loss: 0.1873 - acc: 0.6667 - val_loss: 0.4104 - val_acc: 0.8270\n",
      "Epoch 9/20\n",
      " - 16s - loss: 0.1688 - acc: 0.6780 - val_loss: 0.4277 - val_acc: 0.8246\n",
      "Epoch 10/20\n",
      " - 17s - loss: 0.1554 - acc: 0.6870 - val_loss: 0.4720 - val_acc: 0.8389\n",
      "Epoch 11/20\n",
      " - 16s - loss: 0.1428 - acc: 0.6907 - val_loss: 0.4771 - val_acc: 0.8318\n",
      "Epoch 12/20\n",
      " - 16s - loss: 0.1317 - acc: 0.6979 - val_loss: 0.4721 - val_acc: 0.8424\n",
      "Epoch 13/20\n",
      " - 15s - loss: 0.1141 - acc: 0.7063 - val_loss: 0.4930 - val_acc: 0.8412\n",
      "Epoch 14/20\n",
      " - 16s - loss: 0.1023 - acc: 0.7106 - val_loss: 0.5502 - val_acc: 0.8246\n",
      "Epoch 15/20\n",
      " - 16s - loss: 0.0946 - acc: 0.7141 - val_loss: 0.6179 - val_acc: 0.8270\n",
      "Epoch 16/20\n",
      " - 16s - loss: 0.0937 - acc: 0.7174 - val_loss: 0.5497 - val_acc: 0.8389\n",
      "Epoch 17/20\n",
      " - 16s - loss: 0.0833 - acc: 0.7174 - val_loss: 0.5872 - val_acc: 0.8341\n",
      "Epoch 18/20\n",
      " - 16s - loss: 0.0757 - acc: 0.7196 - val_loss: 0.6544 - val_acc: 0.8353\n",
      "Epoch 19/20\n",
      " - 16s - loss: 0.0734 - acc: 0.7272 - val_loss: 0.6097 - val_acc: 0.8318\n",
      "Epoch 20/20\n",
      " - 15s - loss: 0.0643 - acc: 0.7301 - val_loss: 0.7187 - val_acc: 0.8329\n"
     ]
    }
   ],
   "source": [
    "model_hist_embedding = model.fit(X_train, Y_train_, epochs = 20, batch_size=batch_size, verbose = 2,\n",
    "                        class_weight = [1/Y_train_[:,0].mean(), 1/Y_train_[:,1].mean()],validation_data=(xt,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8329383886255924\n"
     ]
    }
   ],
   "source": [
    "def acc(y,y_):\n",
    "    score = 0\n",
    "    for i in range(y.shape[0]):\n",
    "        if np.argmax(y[i]) == np.argmax(y_[i]):\n",
    "            score+=1\n",
    "    score = score / y.shape[0]\n",
    "    return score\n",
    "\n",
    "pp = model.predict(xt)\n",
    "print(acc(Y_test,pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=5000,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "           n_jobs=-1)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a gradient boosted machine\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "gbm = GradientBoostingClassifier(n_estimators = 5000)\n",
    "mo = MultiOutputClassifier(gbm, n_jobs = -1)\n",
    "mo.fit(X_train,Y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mo.score(xt,Y_test))\n",
    "# print(mo.score(X_train,Y_train_))\n",
    "# predd = mo.predict(X_train)\n",
    "# pred_test = mo.predict(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.17871094  0.32617188 -0.11865234 ...  0.41796875  0.31445312\n",
      "   0.06542969]\n",
      " [-0.06005859 -0.09326172 -0.07226562 ...  0.02392578  0.11083984\n",
      "   0.10107422]\n",
      " ...\n",
      " [ 0.14648438 -0.01379395 -0.01916504 ...  0.04199219  0.16308594\n",
      "   0.02026367]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.01782227  0.32617188 -0.14746094 ... -0.07226562  0.19628906\n",
      "   0.02307129]]\n"
     ]
    }
   ],
   "source": [
    "# print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hist_embedding = model.fit(X_train, Y_train_, epochs = 20, batch_size=batch_size, verbose = 2,\n",
    "                        validation_data=(X_test,Y_test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[126,  89],\n",
       "       [ 52, 577]], dtype=int64)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_test[:,1], np.round(model.predict(xt))[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Accuracy\n",
    "x = np.arange(20)+1\n",
    "fig=plt.figure(dpi=300)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, model_hist_embedding.history['acc'])\n",
    "ax.plot(x, model_hist_embedding.history['val_acc'])\n",
    "ax.legend(['Training', 'Testing'], loc='lower right')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.45,1.01])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"LSTM Accuracy\")\n",
    "plt.show()\n",
    "fig.savefig(fname='03.png', bbox_inches='tight', format='png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_hist_embedding.model.save(\"../wyns/data/climate_sentiment_m6.h5\")\n",
    "# m6 is nans included in the training set but removed from test set, where a nan is [0,0] for [for, against]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"../wyns/data/climate_sentiment_m6.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model_hist_embedding.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_sentiment(tweet):\n",
    "    # Review is coming in as Y/N/NaN\n",
    "    # this then cleans the summary and review and gives it a positive or negative value\n",
    "    norm_text = normalize(tweet[0])\n",
    "    if tweet[1] in ('Yes', 'Y'):\n",
    "        return ['positive', norm_text]\n",
    "    elif tweet[1] in ('No', 'N'):\n",
    "        return ['negative', norm_text]\n",
    "    else:\n",
    "        return ['other', norm_text]\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    norm_text = normalize(tweet[0])\n",
    "    return [tweet[1], tweet[2], norm_text, tweet[3], tweet[4], tweet[5], tweet[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fixed: Take the politics out of climate change and work with other parties to create an independent climate change commission. Climate change is a major environmental issue. We want all policies, economic or other, to enhance and not harm the environment. https://t.co/B9Tl1YoK19</th>\n",
       "      <th>174.161834</th>\n",
       "      <th>-37.292621</th>\n",
       "      <th>2018-06-14 23:51:23</th>\n",
       "      <th>0</th>\n",
       "      <th>Auckland, New Zealand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@rolandscahill @tracymoore1013 I'd say China, ...</td>\n",
       "      <td>-112.323914</td>\n",
       "      <td>33.290260</td>\n",
       "      <td>2018-06-14 23:40:04</td>\n",
       "      <td>0</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We live in #interestingtimes. Antarctica's mel...</td>\n",
       "      <td>174.613267</td>\n",
       "      <td>-41.362455</td>\n",
       "      <td>2018-06-14 23:34:49</td>\n",
       "      <td>0</td>\n",
       "      <td>Wellington City, New Zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Big announcement at  #PECdinner2018: Sec. Cind...</td>\n",
       "      <td>-75.280284</td>\n",
       "      <td>39.871811</td>\n",
       "      <td>2018-06-14 23:14:33</td>\n",
       "      <td>0</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When you've got 70+ MM customers, energy compa...</td>\n",
       "      <td>-87.940033</td>\n",
       "      <td>41.644102</td>\n",
       "      <td>2018-06-14 23:13:56</td>\n",
       "      <td>0</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Seriously addressing climate change in the im...</td>\n",
       "      <td>-123.260264</td>\n",
       "      <td>49.638739</td>\n",
       "      <td>2018-06-14 22:42:30</td>\n",
       "      <td>0</td>\n",
       "      <td>Squamish, British Columbia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Fixed: Take the politics out of climate change and work with other parties to create an independent climate change commission. Climate change is a major environmental issue. We want all policies, economic or other, to enhance and not harm the environment. https://t.co/B9Tl1YoK19  \\\n",
       "0  @rolandscahill @tracymoore1013 I'd say China, ...                                                                                                                                                                                                                                        \n",
       "1  We live in #interestingtimes. Antarctica's mel...                                                                                                                                                                                                                                        \n",
       "2  Big announcement at  #PECdinner2018: Sec. Cind...                                                                                                                                                                                                                                        \n",
       "3  When you've got 70+ MM customers, energy compa...                                                                                                                                                                                                                                        \n",
       "4  “Seriously addressing climate change in the im...                                                                                                                                                                                                                                        \n",
       "\n",
       "   174.161834  -37.292621  2018-06-14 23:51:23  0  \\\n",
       "0 -112.323914   33.290260  2018-06-14 23:40:04  0   \n",
       "1  174.613267  -41.362455  2018-06-14 23:34:49  0   \n",
       "2  -75.280284   39.871811  2018-06-14 23:14:33  0   \n",
       "3  -87.940033   41.644102  2018-06-14 23:13:56  0   \n",
       "4 -123.260264   49.638739  2018-06-14 22:42:30  0   \n",
       "\n",
       "          Auckland, New Zealand  \n",
       "0                   Phoenix, AZ  \n",
       "1  Wellington City, New Zealand  \n",
       "2              Philadelphia, PA  \n",
       "3                   Chicago, IL  \n",
       "4    Squamish, British Columbia  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../wyns/data/tweets.txt\", delimiter=\"~~n~~\", engine=\"python\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for index, row in df.iterrows():\n",
    "    data.append(clean_tweet(row))\n",
    "twitter = pd.DataFrame(data, columns=['long', 'lat', 'clean_text', 'time', 'retweets', 'location','raw_text'], dtype=str)\n",
    "to_predict_ = twitter['clean_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now for a simple bidirectional LSTM algorithm we set our feature sizes and train a tokenizer\n",
    "# First we Tokenize and get the data into a form that the model can read - this is BoW\n",
    "# In this cell we are also going to define some of our hyperparameters\n",
    "\n",
    "max_fatures = 2000\n",
    "max_len=30\n",
    "batch_size = 32\n",
    "embed_dim = 300\n",
    "lstm_out = 140\n",
    "\n",
    "dense_out=len(labels[0]) #length of features\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(to_predict_)\n",
    "to_predict = tokenizer.texts_to_sequences(to_predict_)\n",
    "to_predict = pad_sequences(to_predict, maxlen=max_len, padding='post')\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative predictions: 4903.0\n",
      "positive predictions: 21894.0\n"
     ]
    }
   ],
   "source": [
    "print(\"negative predictions: {}\".format(sum(np.round(predictions)[:,0])))\n",
    "print(\"positive predictions: {}\".format(sum(np.round(predictions)[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26797, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long</th>\n",
       "      <th>lat</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>time</th>\n",
       "      <th>retweets</th>\n",
       "      <th>location</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-112.324</td>\n",
       "      <td>33.2903</td>\n",
       "      <td>i'd say china but since they're the ones who p...</td>\n",
       "      <td>2018-06-14 23:40:04</td>\n",
       "      <td>0</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>0.582745</td>\n",
       "      <td>0.417255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174.613</td>\n",
       "      <td>-41.3625</td>\n",
       "      <td>we live in #interestingtimes antarctica's melt...</td>\n",
       "      <td>2018-06-14 23:34:49</td>\n",
       "      <td>0</td>\n",
       "      <td>Wellington City, New Zealand</td>\n",
       "      <td>0.17913</td>\n",
       "      <td>0.82087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-75.2803</td>\n",
       "      <td>39.8718</td>\n",
       "      <td>big announcement at #pecdinner : sec cindy ada...</td>\n",
       "      <td>2018-06-14 23:14:33</td>\n",
       "      <td>0</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>0.0825765</td>\n",
       "      <td>0.917424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-87.94</td>\n",
       "      <td>41.6441</td>\n",
       "      <td>when you've got + mm customers energy companie...</td>\n",
       "      <td>2018-06-14 23:13:56</td>\n",
       "      <td>0</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>0.00692675</td>\n",
       "      <td>0.993073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-123.26</td>\n",
       "      <td>49.6387</td>\n",
       "      <td>“seriously addressing climate change in the im...</td>\n",
       "      <td>2018-06-14 22:42:30</td>\n",
       "      <td>0</td>\n",
       "      <td>Squamish, British Columbia</td>\n",
       "      <td>0.0479214</td>\n",
       "      <td>0.952079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      long      lat                                         clean_text  \\\n",
       "0 -112.324  33.2903  i'd say china but since they're the ones who p...   \n",
       "1  174.613 -41.3625  we live in #interestingtimes antarctica's melt...   \n",
       "2 -75.2803  39.8718  big announcement at #pecdinner : sec cindy ada...   \n",
       "3   -87.94  41.6441  when you've got + mm customers energy companie...   \n",
       "4  -123.26  49.6387  “seriously addressing climate change in the im...   \n",
       "\n",
       "                  time retweets                      location    negative  \\\n",
       "0  2018-06-14 23:40:04        0                   Phoenix, AZ    0.582745   \n",
       "1  2018-06-14 23:34:49        0  Wellington City, New Zealand     0.17913   \n",
       "2  2018-06-14 23:14:33        0              Philadelphia, PA   0.0825765   \n",
       "3  2018-06-14 23:13:56        0                   Chicago, IL  0.00692675   \n",
       "4  2018-06-14 22:42:30        0    Squamish, British Columbia   0.0479214   \n",
       "\n",
       "   positive  \n",
       "0  0.417255  \n",
       "1   0.82087  \n",
       "2  0.917424  \n",
       "3  0.993073  \n",
       "4  0.952079  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out = pd.DataFrame([twitter['long'], twitter['lat'], twitter['clean_text'],\n",
    "                      twitter['time'], twitter['retweets'], twitter['location'], predictions[:,0], predictions[:,1]]).T\n",
    "df_out = df_out.rename(index=str, columns={\"Unnamed 0\": \"negative\", \"Unnamed 1\": \"positive\"})\n",
    "print(df_out.shape)\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv(\"sample_prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "new_lines = []\n",
    "for i in df_out['clean_text']:\n",
    "    raw = i.split(\" \")\n",
    "    out = \"\"\n",
    "    count=0\n",
    "    for j in raw:\n",
    "        if count < 6:\n",
    "            out += j + \" \"\n",
    "            count += 1\n",
    "        else:\n",
    "            out += j + \"\\n\"\n",
    "            count = 0\n",
    "    new_lines.append(out)\n",
    "df_out['new_lines'] = pd.Series(new_lines)\n",
    "# print(new_lines)\n",
    "#     print(re.sub('( [^ ][^ ][^ ]*),', r'\\1 ', i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long</th>\n",
       "      <th>lat</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>time</th>\n",
       "      <th>retweets</th>\n",
       "      <th>location</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>new_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-112.324</td>\n",
       "      <td>33.2903</td>\n",
       "      <td>i'd say china but since they're the ones who perpetrated that climate change hoax my money is on no korea they have better beaches for those hotels trumplethinskin wants to build</td>\n",
       "      <td>2018-06-14 23:40:04</td>\n",
       "      <td>0</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>0.00498438</td>\n",
       "      <td>0.995016</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174.613</td>\n",
       "      <td>-41.3625</td>\n",
       "      <td>we live in #interestingtimes antarctica's melt continues &amp;amp; volcanoes add hot lava to sea &amp;amp; earthquakes raise the sea floor creating an interesting future for coastal new zealand will #wellington be ready?</td>\n",
       "      <td>2018-06-14 23:34:49</td>\n",
       "      <td>0</td>\n",
       "      <td>Wellington City, New Zealand</td>\n",
       "      <td>0.00104104</td>\n",
       "      <td>0.998959</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-75.2803</td>\n",
       "      <td>39.8718</td>\n",
       "      <td>big announcement at #pecdinner : sec cindy adams dunn unveils climate change resiliency plan for pa public lands</td>\n",
       "      <td>2018-06-14 23:14:33</td>\n",
       "      <td>0</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>0.000292582</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-87.94</td>\n",
       "      <td>41.6441</td>\n",
       "      <td>when you've got + mm customers energy companies will respond this is how america is moving on #climateaction</td>\n",
       "      <td>2018-06-14 23:13:56</td>\n",
       "      <td>0</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>0.895099</td>\n",
       "      <td>0.104901</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-123.26</td>\n",
       "      <td>49.6387</td>\n",
       "      <td>“seriously addressing climate change in the immediate future demands not a theoretically effective strategy but an actually effective one”: why carbon pricing isn’t working via</td>\n",
       "      <td>2018-06-14 22:42:30</td>\n",
       "      <td>0</td>\n",
       "      <td>Squamish, British Columbia</td>\n",
       "      <td>0.19368</td>\n",
       "      <td>0.80632</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      long      lat  \\\n",
       "0 -112.324  33.2903   \n",
       "1  174.613 -41.3625   \n",
       "2 -75.2803  39.8718   \n",
       "3   -87.94  41.6441   \n",
       "4  -123.26  49.6387   \n",
       "\n",
       "                                                                                                                                                                                                              clean_text  \\\n",
       "0                                     i'd say china but since they're the ones who perpetrated that climate change hoax my money is on no korea they have better beaches for those hotels trumplethinskin wants to build   \n",
       "1  we live in #interestingtimes antarctica's melt continues &amp; volcanoes add hot lava to sea &amp; earthquakes raise the sea floor creating an interesting future for coastal new zealand will #wellington be ready?    \n",
       "2                                                                                                       big announcement at #pecdinner : sec cindy adams dunn unveils climate change resiliency plan for pa public lands   \n",
       "3                                                                                                          when you've got + mm customers energy companies will respond this is how america is moving on #climateaction    \n",
       "4                                      “seriously addressing climate change in the immediate future demands not a theoretically effective strategy but an actually effective one”: why carbon pricing isn’t working via    \n",
       "\n",
       "                  time retweets                      location     negative  \\\n",
       "0  2018-06-14 23:40:04        0                   Phoenix, AZ   0.00498438   \n",
       "1  2018-06-14 23:34:49        0  Wellington City, New Zealand   0.00104104   \n",
       "2  2018-06-14 23:14:33        0              Philadelphia, PA  0.000292582   \n",
       "3  2018-06-14 23:13:56        0                   Chicago, IL     0.895099   \n",
       "4  2018-06-14 22:42:30        0    Squamish, British Columbia      0.19368   \n",
       "\n",
       "   positive new_lines  \n",
       "0  0.995016       NaN  \n",
       "1  0.998959       NaN  \n",
       "2  0.999707       NaN  \n",
       "3  0.104901       NaN  \n",
       "4   0.80632       NaN  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16268"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
